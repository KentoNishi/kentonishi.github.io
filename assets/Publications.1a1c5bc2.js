import{C as tt,D as et,E as st,S as T,i as W,s as E,F as j,e as u,b as r,j as O,c as d,d as p,G as D,H,I as M,q as k,u as y,f as h,g as w,A as nt,B as lt,J as at,K as ot,L as it,o as R,p as q,k as z,M as rt,v as V,t as G,l as A,n as B,r as U}from"./index.79f42759.js";function ct(i){const t=et(i,{borderStyle:"hover",disableClicking:!0}),e=st(i);return e.innerHTML=`
    .${e.id} {
      display: flex;
      align-items: center;
      justify-content: center;
      padding: 0px;
    }
  `,tt(t.destroy,e.remove)}function ut(i){let t,e,l;const o=i[1].default,s=j(o,i,i[0],null);return{c(){t=u("div"),e=u("div"),s&&s.c(),r(e,"class","grid svelte-115zgra"),r(t,"class","wrap svelte-115zgra"),O(t,"transform-origin","top center")},m(n,a){d(n,t,a),p(t,e),s&&s.m(e,null),l=!0},p(n,[a]){s&&s.p&&(!l||a&1)&&D(s,o,n,n[0],l?M(o,n[0],a,null):H(n[0]),null)},i(n){l||(k(s,n),l=!0)},o(n){y(s,n),l=!1},d(n){n&&h(t),s&&s.d(n)}}}function ft(i,t,e){let{$$slots:l={},$$scope:o}=t;return i.$$set=s=>{"$$scope"in s&&e(0,o=s.$$scope)},[o,l]}class pt extends T{constructor(t){super(),W(this,t,ft,ut,E,{})}}const mt=i=>({}),Z=i=>({}),dt=i=>({}),Q=i=>({}),ht=i=>({}),X=i=>({});function _t(i){let t,e;const l=i[3].title,o=j(l,i,i[2],Q);return{c(){t=u("h2"),o&&o.c(),r(t,"class","svelte-jpiz1u")},m(s,n){d(s,t,n),o&&o.m(t,null),e=!0},p(s,n){o&&o.p&&(!e||n&4)&&D(o,l,s,s[2],e?M(l,s[2],n,dt):H(s[2]),Q)},i(s){e||(k(o,s),e=!0)},o(s){y(o,s),e=!1},d(s){s&&h(t),o&&o.d(s)}}}function gt(i){let t,e;const l=i[3].title,o=j(l,i,i[2],X);return{c(){t=u("h1"),o&&o.c(),r(t,"class","svelte-jpiz1u")},m(s,n){d(s,t,n),o&&o.m(t,null),e=!0},p(s,n){o&&o.p&&(!e||n&4)&&D(o,l,s,s[2],e?M(l,s[2],n,ht):H(s[2]),X)},i(s){e||(k(o,s),e=!0)},o(s){y(o,s),e=!1},d(s){s&&h(t),o&&o.d(s)}}}function vt(i){let t,e,l,o,s,n,a,c;const m=[gt,_t],_=[];function v(f,$){return f[0]?0:1}l=v(i),o=_[l]=m[l](i);const C=i[3].content,g=j(C,i,i[2],Z);return{c(){t=u("div"),e=u("div"),o.c(),s=w(),n=u("p"),g&&g.c(),r(n,"class","svelte-jpiz1u"),r(e,"class","card svelte-jpiz1u"),r(e,"style",a=i[0]?"align-items: center; background-color: transparent;":""),r(t,"class","parent svelte-jpiz1u")},m(f,$){d(f,t,$),p(t,e),_[l].m(e,null),p(e,s),p(e,n),g&&g.m(n,null),i[4](e),c=!0},p(f,[$]){let I=l;l=v(f),l===I?_[l].p(f,$):(nt(),y(_[I],1,1,()=>{_[I]=null}),lt(),o=_[l],o?o.p(f,$):(o=_[l]=m[l](f),o.c()),k(o,1),o.m(e,s)),g&&g.p&&(!c||$&4)&&D(g,C,f,f[2],c?M(C,f[2],$,mt):H(f[2]),Z),(!c||$&1&&a!==(a=f[0]?"align-items: center; background-color: transparent;":""))&&r(e,"style",a)},i(f){c||(k(o),k(g,f),c=!0)},o(f){y(o),y(g,f),c=!1},d(f){f&&h(t),_[l].d(),g&&g.d(f),i[4](null)}}}function bt(i,t,e){let{$$slots:l={},$$scope:o}=t,{titleCard:s=!1}=t,n,a;at(()=>{s||(a=ct(n))}),ot(()=>{a&&a.destroy()});function c(m){it[m?"unshift":"push"](()=>{n=m,e(1,n)})}return i.$$set=m=>{"titleCard"in m&&e(0,s=m.titleCard),"$$scope"in m&&e(2,o=m.$$scope)},[s,n,o,l,c]}class K extends T{constructor(t){super(),W(this,t,bt,vt,E,{titleCard:0})}}function $t(i){let t;return{c(){t=G("Research and Publications")},m(e,l){d(e,t,l)},d(e){e&&h(t)}}}function wt(i){let t,e,l,o,s,n,a,c,m,_,v;return{c(){t=u("div"),e=u("a"),l=u("button"),l.textContent="Google Scholar",o=w(),s=u("a"),n=u("button"),n.textContent="Semantic Scholar",a=w(),c=u("a"),m=u("button"),m.textContent="Papers with Code",r(l,"class","svelte-1ctchzr"),r(e,"href","https://scholar.google.com/citations?user=iQoZSr4AAAAJ"),r(e,"target","_blank"),r(e,"class","svelte-1ctchzr"),r(n,"class","svelte-1ctchzr"),r(s,"href","https://www.semanticscholar.org/author/Kento-Nishi/1721045940"),r(s,"target","_blank"),r(s,"class","svelte-1ctchzr"),r(m,"class","svelte-1ctchzr"),r(c,"href","https://paperswithcode.com/search?q=author%3AKento+Nishi"),r(c,"target","_blank"),r(c,"class","svelte-1ctchzr"),r(t,"class","buttons svelte-1ctchzr"),O(t,"margin-top","0.5rem")},m(C,g){d(C,t,g),p(t,e),p(e,l),p(t,o),p(t,s),p(s,n),p(t,a),p(t,c),p(c,m),_||(v=[z(A.call(null,l)),z(A.call(null,n)),z(A.call(null,m))],_=!0)},p:B,d(C){C&&h(t),_=!1,U(v)}}}function Ct(i){let t;return{c(){t=G("Augmentation Strategies for Learning with Noisy Labels")},m(e,l){d(e,t,l)},d(e){e&&h(t)}}}function kt(i){let t,e,l,o,s,n,a,c,m,_,v,C,g,f,$,I,L,P,J,x,N,F,Y;return{c(){t=u("i"),t.textContent="Conference on Computer Vision and Pattern Recognition (2021)",e=w(),l=u("p"),l.innerHTML=`<a href="mailto:kento24gs@outlook.com" class="svelte-1ctchzr">Kento Nishi</a>*,
          <a href="mailto:yding@cs.ucsb.edu" class="svelte-1ctchzr">Yi Ding</a>*,
          <a href="mailto:anrich@cs.ucsb.edu" class="svelte-1ctchzr">Alex Rich</a>,
          <a href="mailto:holl@cs.ucsb.edu" class="svelte-1ctchzr">Tobias H\xF6llerer</a>
          [<code>*</code>:
          <a href="mailto:kento24gs@outlook.com,yding@cs.ucsb.edu" class="svelte-1ctchzr">equal contribution</a>]`,o=w(),s=u("p"),s.textContent=`Imperfect labels are ubiquitous in real-world datasets. Several recent
          successful methods for training deep neural networks (DNNs) robust to
          label noise have used two primary techniques: filtering samples based
          on loss during a warm-up phase to curate an initial set of cleanly
          labeled samples, and using the output of a network as a pseudo-label
          for subsequent loss calculations. In this paper, we evaluate different
          augmentation strategies for algorithms tackling the "learning with
          noisy labels" problem. We propose and examine multiple augmentation
          strategies and evaluate them using synthetic datasets based on
          CIFAR-10 and CIFAR-100, as well as on the real-world dataset
          Clothing1M. Due to several commonalities in these algorithms, we find
          that using one set of augmentations for loss modeling tasks and
          another set for learning is the most effective, improving results on
          the state-of-the-art and other previous methods. Furthermore, we find
          that applying augmentation during the warm-up period can negatively
          impact the loss convergence behavior of correctly versus incorrectly
          labeled samples. We introduce this augmentation strategy to the
          state-of-the-art technique and demonstrate that we can improve
          performance across all evaluated noise levels. In particular, we
          improve accuracy on the CIFAR-10 benchmark at 90% symmetric noise by
          more than 15% in absolute accuracy, and we also improve performance on
          the real-world dataset Clothing1M.`,n=w(),a=u("div"),c=u("a"),m=u("button"),m.textContent="View on arXiv",_=w(),v=u("a"),C=u("button"),C.textContent="View PDF",g=w(),f=u("a"),$=u("button"),$.textContent="View code on GitHub",I=w(),L=u("a"),P=u("button"),P.textContent="View on Papers with Code",J=w(),x=u("a"),N=u("button"),N.textContent="Watch CVPR Video",r(m,"class","svelte-1ctchzr"),r(c,"href","https://arxiv.org/abs/2103.02130"),r(c,"target","_blank"),r(c,"class","svelte-1ctchzr"),r(C,"class","svelte-1ctchzr"),r(v,"href","https://arxiv.org/pdf/2103.02130.pdf"),r(v,"target","_blank"),r(v,"class","svelte-1ctchzr"),r($,"class","svelte-1ctchzr"),r(f,"href","https://github.com/KentoNishi/Augmentation-for-LNL"),r(f,"target","_blank"),r(f,"class","svelte-1ctchzr"),r(P,"class","svelte-1ctchzr"),r(L,"href","https://paperswithcode.com/paper/augmentation-strategies-for-learning-with"),r(L,"target","_blank"),r(L,"class","svelte-1ctchzr"),r(N,"class","svelte-1ctchzr"),r(x,"href","https://kentonishi.github.io/Augmentation-for-LNL/CVPR_Video.mp4"),r(x,"target","_blank"),r(x,"class","svelte-1ctchzr"),r(a,"class","buttons svelte-1ctchzr")},m(b,S){d(b,t,S),d(b,e,S),d(b,l,S),d(b,o,S),d(b,s,S),d(b,n,S),d(b,a,S),p(a,c),p(c,m),p(a,_),p(a,v),p(v,C),p(a,g),p(a,f),p(f,$),p(a,I),p(a,L),p(L,P),p(a,J),p(a,x),p(x,N),F||(Y=[z(A.call(null,m)),z(A.call(null,C)),z(A.call(null,$)),z(A.call(null,P)),z(A.call(null,N))],F=!0)},p:B,d(b){b&&h(t),b&&h(e),b&&h(l),b&&h(o),b&&h(s),b&&h(n),b&&h(a),F=!1,U(Y)}}}function yt(i){let t;return{c(){t=G(`Improving Label Noise Robustness with Data Augmentation and
        Semi-Supervised Learning (Student Abstract)`)},m(e,l){d(e,t,l)},d(e){e&&h(t)}}}function zt(i){let t,e,l,o,s;return{c(){t=u("i"),t.textContent="Proceedings of the AAAI Conference on Artificial Intelligence (2021)",e=w(),l=u("p"),l.innerHTML=`<a href="mailto:kento24gs@outlook.com" class="svelte-1ctchzr">Kento Nishi</a>,
          <a href="mailto:yding@cs.ucsb.edu" class="svelte-1ctchzr">Yi Ding</a>,
          <a href="mailto:anrich@cs.ucsb.edu" class="svelte-1ctchzr">Alex Rich</a>,
          <a href="mailto:holl@cs.ucsb.edu" class="svelte-1ctchzr">Tobias H\xF6llerer</a>`,o=w(),s=u("p"),s.textContent=`Modern machine learning algorithms typically require large amounts of
          labeled training data to fit a reliable model. To minimize the cost of
          data collection, researchers often employ techniques such as
          crowdsourcing and web scraping. However, web data and human
          annotations are known to exhibit high margins of error, resulting in
          sizable amounts of incorrect labels. Poorly labeled training data can
          cause models to overfit to the noise distribution, crippling
          performance in real-world applications. In this work, we investigate
          the viability of using data augmentation in conjunction with
          semi-supervised learning to improve the label noise robustness of
          image classification models. We conduct several experiments using
          noisy variants of the CIFAR-10 image classification dataset to
          benchmark our method against existing algorithms. Experimental results
          show that our augmentative SSL approach improves upon the
          state-of-the-art.`},m(n,a){d(n,t,a),d(n,e,a),d(n,l,a),d(n,o,a),d(n,s,a)},p:B,d(n){n&&h(t),n&&h(e),n&&h(l),n&&h(o),n&&h(s)}}}function At(i){let t,e,l,o,s,n;return t=new K({props:{titleCard:!0,$$slots:{content:[wt],title:[$t]},$$scope:{ctx:i}}}),l=new K({props:{$$slots:{content:[kt],title:[Ct]},$$scope:{ctx:i}}}),s=new K({props:{$$slots:{content:[zt],title:[yt]},$$scope:{ctx:i}}}),{c(){R(t.$$.fragment),e=w(),R(l.$$.fragment),o=w(),R(s.$$.fragment)},m(a,c){q(t,a,c),d(a,e,c),q(l,a,c),d(a,o,c),q(s,a,c),n=!0},p(a,c){const m={};c&1&&(m.$$scope={dirty:c,ctx:a}),t.$set(m);const _={};c&1&&(_.$$scope={dirty:c,ctx:a}),l.$set(_);const v={};c&1&&(v.$$scope={dirty:c,ctx:a}),s.$set(v)},i(a){n||(k(t.$$.fragment,a),k(l.$$.fragment,a),k(s.$$.fragment,a),n=!0)},o(a){y(t.$$.fragment,a),y(l.$$.fragment,a),y(s.$$.fragment,a),n=!1},d(a){V(t,a),a&&h(e),V(l,a),a&&h(o),V(s,a)}}}function St(i){let t,e,l,o,s;return e=new pt({props:{$$slots:{default:[At]},$$scope:{ctx:i}}}),{c(){t=u("div"),R(e.$$.fragment)},m(n,a){d(n,t,a),q(e,t,null),l=!0,o||(s=z(rt.call(null,t)),o=!0)},p(n,[a]){const c={};a&1&&(c.$$scope={dirty:a,ctx:n}),e.$set(c)},i(n){l||(k(e.$$.fragment,n),l=!0)},o(n){y(e.$$.fragment,n),l=!1},d(n){n&&h(t),V(e),o=!1,s()}}}class Lt extends T{constructor(t){super(),W(this,t,null,St,E,{})}}export{Lt as default};
